{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\"ğŸ“Œ CHEAT SHEET 1: FORECASTING (Time Series Analysis)\n1ï¸âƒ£ What is Time Series?\n\nData collected over time (daily, monthly, yearly).\nExamples:\n\nStock prices\n\nTemperature\n\nSales data\n\nWebsite traffic\n\nâ­ COMPONENTS OF TIME SERIES\nComponent\tMeaning\nTrend\tLong-term increase or decrease\nSeasonality\tRepeated patterns (diwali, winter, weekend)\nCyclic\tUp-down movements (business cycles)\nIrregular/Noise\tRandom fluctuations\nâ­ TYPES OF FORECASTING METHODS\n1. Naive Forecast\nNext Value = Last Value\n\n\nSimple baseline model\n\n2. Moving Average (MA)\n\nSmooth data using average of previous N values.\n\nFormula:\n\nMA_t = (X_{t-1} + X_{t-2} + ... + X_{t-N}) / N\n\n3. Exponential Smoothing (SES)\n\nUseful for no-trend, no-seasonality data.\n\nF_t = Î±X_{t-1} + (1-Î±)F_{t-1}\n\n\nÎ± = smoothing constant\n\n4. Holtâ€™s Linear Trend Model\n\nForecast = Level + Trend\n\n5. Holt-Winters Method\n\nUsed for:\nâœ” Trend\nâœ” Seasonality\nâœ” Cyclic patterns\n\nTwo variations:\n\nAdditive\n\nMultiplicative\n\n6. ARIMA Model\n\nMost important model.\n\nARIMA = AutoRegressive + Integration + Moving Average\n\nParameters:\n\nAR (p) â†’ depends on lag values\n\nI (d) â†’ differencing\n\nMA (q) â†’ depends on lagged error\n\nModel:\n\nARIMA(p, d, q)\n\nâ­ MODEL SELECTION TOOLS\n\nACF (Autocorrelation)\n\nPACF (Partial Autocorrelation)\n\nStationarity: ADF test\n\nSeasonal decomposition\n\nRMSE, MAPE, MAE\n\nâ­ Accuracy Metrics\nMAPE = Mean Absolute Percentage Error\nRMSE = Root Mean Square Error\nMAE  = Mean Absolute Error\n\nğŸ“Œ CHEAT SHEET 2: K-MEANS CLUSTERING\n1ï¸âƒ£ What is K-Means?\n\nUnsupervised ML algorithm used to group similar data points into K clusters.\n\n2ï¸âƒ£ How K-Means Works (VERY IMPORTANT)\n\nChoose number of clusters K\n\nRandomly select K centroids\n\nAssign each point to nearest centroid\n\nRecalculate centroids\n\nRepeat until stable (convergence)\n\n3ï¸âƒ£ Distance Used\n\nDefault:\n\nEuclidean Distance\n\nâ­ FORMULA FOR CENTROID\nCentroid = Average of all points in the cluster\n\nâ­ Choosing K: Elbow Method (MOST ASKED)\n\nPlot:\nX-axis â†’ Number of clusters (k)\nY-axis â†’ Inertia (within-cluster sum of squares)\n\nBend point = Best K\n\n4ï¸âƒ£ Advantages\n\nSimple\n\nFast\n\nWorks well on large datasets\n\n5ï¸âƒ£ Disadvantages\n\nSensitive to outliers\n\nWorks only with numeric data\n\nNeed to pre-choose K\n\nBad for non-spherical clusters\n\nâ­ WHEN TO USE K-MEANS?\n\nâœ” Customer segmentation\nâœ” Image compression\nâœ” Social media grouping\nâœ” Anomaly detection\nâœ” Market segmentation\n\nâ­ SKLEARN CODE TEMPLATE (VERY IMPORTANT)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(df)\n\nlabels = kmeans.labels_\ncenters = kmeans.cluster_centers_\n\nâ­ Accuracy / Validation\n\nInertia (WCSS)\n\nSilhouette Score\n\n-1 to +1  \nHigher = better clusters\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_score(df, labels)\"\"\"\"\n\n\n\n\n\n\n\"\"\"1. K-Means belongs to which type of learning?\n\nA. Supervised\nB. Unsupervised\nC. Reinforcement\nD. Semi-supervised\nAnswer: B\n\n2. What does â€œKâ€ in K-Means represent?\n\nA. Data points\nB. Distance metric\nC. Number of clusters\nD. Cluster size\nAnswer: C\n\n3. Which distance metric is default in K-Means?\n\nA. Manhattan\nB. Hamming\nC. Euclidean\nD. Minkowski\nAnswer: C\n\n4. K-Means tries to minimize which value?\n\nA. Accuracy\nB. Variance\nC. WCSS (Within Cluster Sum of Squares)\nD. Gradient\nAnswer: C\n\n5. What is a centroid?\n\nA. Random point\nB. Farthest point\nC. Mean of all points in cluster\nD. Median of data\nAnswer: C\n\n6. K-Means fails badly when data hasâ€”\n\nA. Spherical clusters\nB. Outliers\nC. Equal densities\nD. Few dimensions\nAnswer: B\n\n7. K-Means uses which optimization technique?\n\nA. Gradient descent\nB. Lloydâ€™s Algorithm\nC. Backpropagation\nD. A* search\nAnswer: B\n\n8. Which initialization technique reduces randomness?\n\nA. K-Means++\nB. Random init\nC. K-Nearest\nD. Silhouette init\nAnswer: A\n\n9. Elbow method is used toâ€”\n\nA. Remove outliers\nB. Choose distance metric\nC. Select K\nD. Normalize data\nAnswer: C\n\n10. Inertia isâ€”\n\nA. Sum of distances from centroid\nB. Sum of squared distances within cluster\nC. Distance between clusters\nD. Cluster density\nAnswer: B\n\n11. Increasing K always decreasesâ€”\n\nA. Accuracy\nB. Inertia\nC. Silhouette score\nD. Centroids\nAnswer: B\n\n12. K-Means works only onâ€”\n\nA. Numerical data\nB. Text data\nC. Audio\nD. Images only\nAnswer: A\n\n13. K-Means complexity isâ€”\n\nA. O(n)\nB. O(nÂ²)\nC. O(n Ã— k Ã— t)\nD. O(kÂ³)\nAnswer: C\n\n14. K-Means converges whenâ€”\n\nA. Accuracy reaches 100%\nB. Centroids stop moving\nC. Inertia increases\nD. New clusters appear\nAnswer: B\n\n15. K-Means is sensitive toâ€”\n\nA. Scaling\nB. Trainâ€“test split\nC. Loss function\nD. Batch size\nAnswer: A\n\n16. Silhouette score measuresâ€”\n\nA. Outlier ratio\nB. Clustering quality\nC. Cluster count\nD. Model speed\nAnswer: B\n\n17. Silhouette score ranges betweenâ€”\n\nA. -10 to +10\nB. -1 to +1\nC. 0 to 100\nD. -5 to +5\nAnswer: B\n\n18. K-Means fails for clusters shaped likeâ€”\n\nA. Round blobs\nB. Linear lines\nC. Concentric circles\nD. Gaussian clusters\nAnswer: C\n\n19. Which step comes first in K-Means?\n\nA. Assign clusters\nB. Recompute centroids\nC. Randomly initialize K centroids\nD. Normalize data\nAnswer: C\n\n20. K-Means++ helps inâ€”\n\nA. Faster convergence\nB. Better accuracy\nC. Better centroid initialization\nD. All of the above\nAnswer: D\n\nğŸ¯ FORECASTING / TIME SERIES â€” 20 MCQs (AWS ML COURSE STYLE)\n\nCovers AWS Forecasting content: ARIMA, trend, seasonality, metrics, ACF/PACF, smoothing, decomposition, stationarity, MAPE, RMSE.\n\n1. Time series forecasting is mainly used forâ€”\n\nA. Image recognition\nB. Sequential data prediction\nC. Clustering\nD. Reinforcement learning\nAnswer: B\n\n2. Which component represents repeated patterns like seasons?\n\nA. Trend\nB. Noise\nC. Seasonality\nD. Cyclic\nAnswer: C\n\n3. Which test checks stationarity?\n\nA. T-test\nB. Chi-square\nC. Augmented Dickey-Fuller\nD. ANOVA\nAnswer: C\n\n4. ARIMA stands forâ€”\n\nA. Automated Regression Inference Mean Analysis\nB. AutoRegressive Integrated Moving Average\nC. Artificial Recursive Integrated Matrix Analysis\nD. Advanced Regression Iteration Moving Algorithm\nAnswer: B\n\n5. In ARIMA(p, d, q), â€˜dâ€™ representsâ€”\n\nA. Trend\nB. Differencing order\nC. Seasonality\nD. Noise\nAnswer: B\n\n6. ACF plot helps identifyâ€”\n\nA. MA terms\nB. AR terms\nC. Residual errors\nD. Noise\nAnswer: A\n\n7. PACF plot helps identifyâ€”\n\nA. MA terms\nB. AR terms\nC. Trend\nD. Stationarity\nAnswer: B\n\n8. RMSE formula includesâ€”\n\nA. Mean absolute values\nB. Squared error\nC. Exponential smoothing\nD. ACF\nAnswer: B\n\n9. Best model is the one which minimizesâ€”\n\nA. AIC\nB. RMSE\nC. MAPE\nD. All\nAnswer: D\n\n10. Exponential smoothing usesâ€”\n\nA. Equal weights\nB. More weight to recent data\nC. More weight to older data\nD. No weights\nAnswer: B\n\n11. Holt-Winters is used when data hasâ€”\n\nA. Only trend\nB. Only seasonality\nC. Both trend & seasonality\nD. Neither\nAnswer: C\n\n12. Seasonal ARIMA is represented asâ€”\n\nA. SARIMAX\nB. ARIMA*\nC. ARIMAS\nD. S-ARIMAX\nAnswer: A\n\n13. Non-stationary data needsâ€”\n\nA. Scaling\nB. Differencing\nC. Clustering\nD. PCA\nAnswer: B\n\n14. White noise meansâ€”\n\nA. Has trend\nB. Has seasonality\nC. Completely random\nD. Cyclic\nAnswer: C\n\n15. Which metric expresses percentage error?\n\nA. RMSE\nB. MAE\nC. AIC\nD. MAPE\nAnswer: D\n\n16. Amazon Forecast usesâ€”\n\nA. DeepAR\nB. ARIMA\nC. Prophet\nD. All of the above\nAnswer: D\n\n17. When we see a sharp seasonal spike, we should applyâ€”\n\nA. Differencing\nB. Seasonal differencing\nC. PCA\nD. Logistic regression\nAnswer: B\n\n18. Which model uses LSTM-based forecasting?\n\nA. SARIMA\nB. DeepAR (AWS)\nC. Holt-Winters\nD. Naive forecast\nAnswer: B\n\n19. Moving average reducesâ€”\n\nA. Seasonality\nB. Trend\nC. Noise\nD. Stationarity\nAnswer: C\n\n20. Forecast horizon meansâ€”\n\nA. Dataset size\nB. Number of future periods to predict\nC. Cluster size\nD. ACF range\nAnswer: B\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nğŸ“Œ K-Means Clustering â€“ Complete Advanced Cheat Sheet\nâœ… 1. What is K-Means?\n\nUnsupervised ML algorithm jo data ko K clusters me divide karta hai.\n\nGoal â†’ Minimize Within-Cluster-Sum-of-Squares (WCSS)\n\nğ‘Š\nğ¶\nğ‘†\nğ‘†\n=\nÎ£\n(\nğ‘‘\nğ‘–\nğ‘ \nğ‘¡\nğ‘\nğ‘›\nğ‘\nğ‘’\nğ‘œ\nğ‘“\nğ‘’\nğ‘\nğ‘\nâ„\nğ‘\nğ‘œ\nğ‘–\nğ‘›\nğ‘¡\nğ‘“\nğ‘Ÿ\nğ‘œ\nğ‘š\nğ‘–\nğ‘¡\nğ‘ \nğ‘\nğ‘’\nğ‘›\nğ‘¡\nğ‘Ÿ\nğ‘œ\nğ‘–\nğ‘‘\n)\nWCSS=Î£(distanceofeachpointfromitscentroid)\n\nâœ… 2. K-Means ka Working (Step-by-Step)\n\nChoose number of clusters K\n\nInitialize K centroids randomly\n\nAssign each point â†’ nearest centroid\n(Distance = Euclidean usually)\n\nRecalculate new centroid\n(mean of all points in cluster)\n\nRepeat until:\n\ncentroids stabilize OR\n\nmax iterations reached\n\nâœ… 3. When to Use K-Means\n\nWhen cluster boundaries are spherical\n\nFeatures are numeric\n\nDataset is not too noisy\n\nCluster sizes similar hon\n\nâŒ When NOT to Use K-Means\n\nNon-spherical clusters\n\nVarying density\n\nOutliers bohot zyada\n\nCategorical data only\n\nCluster sizes uneven\n\nğŸ§® 4. Distance Metrics\nDistance\tUsed For\nEuclidean\tDefault, numeric data\nManhattan\thigh dimensional\nCosine\ttext / similarity\nğŸ”§ 5. Hyperparameters\nParameter\tMeaning\nn_clusters\tK value\ninit\tcentroid initialization (k-means++ recommended)\nmax_iter\tmax iterations per run\nn_init\tnumber of centroid restarts\ntol\tMinimum improvement threshold\nğŸ¯ 6. Elbow Method\n\nBest K find karne ke liye:\n\nPlot K vs WCSS\nBest point = â€œbendâ€ / elbow\n\nğŸ¯ 7. Silhouette Score\n\nMeasures cluster quality:\nRange: -1 to +1\n\n+1 â†’ perfect cluster\n\n0 â†’ overlapping\n\n-1 â†’ wrong cluster\n\nFormula:\n\nğ‘ \n=\nğ‘\nâˆ’\nğ‘\nğ‘š\nğ‘\nğ‘¥\n(\nğ‘\n,\nğ‘\n)\ns=\nmax(a,b)\nbâˆ’a\n\tâ€‹\n\n\nğŸ“Š 8. K-Means Assumptions\n\nMean is valid representation (numeric features)\n\nClusters are spherical\n\nSimilar variance across clusters\n\nğŸ§© 9. Common Problems\nProblem\tFix\nWrong K\tUse elbow, silhouette\nPoor initialization\tUse k-means++\nSensitive to outliers\tUse RobustScaler / remove outliers\nNon-spherical clusters\tTry DBSCAN / Hierarchical\nğŸ“Œ 10. Tips\n\nAlways scale data\n\nTry multiple n_init (20â€“50)\n\nRemove outliers\n\nTry PCA before K-Means\n\nğŸ“Œ Time Series Forecasting â€“ Complete Advanced Cheat Sheet\nğŸ”¥ 1. What is Time Series?\n\nData collected in time order.\n\nExamples:\n\nSales\n\nStock prices\n\nEnergy consumption\n\nWeather\n\nğŸ“Œ 2. Components of Time Series\nComponent\tExplanation\nTrend\tlong-term increase/decrease\nSeasonality\trepeating pattern (daily, weekly, yearly)\nCyclic\tirregular cycles\nNoise\trandomness\nğŸ§ª 3. Stationarity\n\nTime series is stationary when:\n\nConstant mean\n\nConstant variance\n\nConstant autocovariance\n\nADF test used to check.\n\nğŸ”§ 4. Common Forecasting Models\nâœ” Classical Models\n\nAR (Auto Regression)\n\nMA (Moving Average)\n\nARMA\n\nARIMA\n\nSARIMA\n\nSARIMAX (with external regressors)\n\nâœ” ML Models\n\nLinear Regression\n\nRandom Forest Regressor\n\nXGBoost / LightGBM\n\nSVR\n\nâœ” Deep Learning\n\nLSTM\n\nGRU\n\nBidirectional LSTM\n\nTemporal Convolutional Networks (TCN)\n\nTransformers (most advanced)\n\nğŸ”¢ 5. ARIMA Formula\n\nARIMA(p,d,q)\n\np = AR terms\n\nd = differencing\n\nq = MA terms\n\nSARIMA = ARIMA + Seasonality (P,D,Q,s)\n\nğŸ“Š 6. Forecasting Steps\n\nLoad data\n\nConvert to datetime index\n\nHandle missing values\n\nCheck stationarity (ADF test)\n\nDecompose time series\n\nACF / PACF plots\n\nBuild ARIMA/SARIMA model\n\nResidual analysis\n\nForecast\n\nEvaluate performance\n\nğŸ§® 7. Evaluation Metrics\nMetric\tUse\nMAE\tabsolute error\nMSE\tpenalizes large error\nRMSE\tstandard scale\nMAPE\t% error\nğŸ”§ 8. Feature Engineering\n\nLag features\n\nRolling mean\n\nRolling std\n\nExponential smoothing\n\nHoliday features\n\nFourier terms (seasonality)\n\nâš ï¸ 9. Common Problems\nProblem\tFix\nNon-stationary\tDifferencing, log transform\nHigh seasonality\tSARIMA / Prophet\nMissing values\tInterpolation\nOutliers\tWinsorization\nğŸ§© 10. AWS ML Course â€” Focus Topics\n\nAutocorrelation\n\nLag features\n\nWindowing\n\nTrain-test split based on time\n\nRolling forecasts\n\nError metrics (MAE, RMSE)\n\nScaling and inverse transformation\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T21:58:26.866717Z","iopub.execute_input":"2025-11-20T21:58:26.867004Z","iopub.status.idle":"2025-11-20T21:58:31.988841Z","shell.execute_reply.started":"2025-11-20T21:58:26.866975Z","shell.execute_reply":"2025-11-20T21:58:31.987484Z"}},"outputs":[],"execution_count":1}]}